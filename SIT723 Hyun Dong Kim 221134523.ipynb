{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YftB7WnbkrX",
        "outputId": "32a0643c-80d2-45e7-91c2-b8948e621d9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Softmax, Dropout, Bidirectional\n",
        "import scipy.io  # to load MAT files\n",
        "import time\n",
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sys\n",
        "import scipy.signal  # to downsample input data\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "\n",
        "print('Tf Keras:', keras.__version__)\n",
        "print('TensorFlow:', tf.__version__)\n",
        "print('GPU device:', tf.test.gpu_device_name())\n",
        "\n",
        "if 'google.colab' in sys.modules:  # try to detect if we're running in colab or locally\n",
        "  working_dir = '/content/drive/MyDrive/dataset'\n",
        "else:\n",
        "  working_dir = '.'\n",
        "\n",
        "log_dir_base = working_dir + '/logs/fit'\n",
        "\n",
        "\n",
        "def create_dataset(window, overlap, decimation_factor = 0, distrib_stat = 0):\n",
        "  # Create the input and target data from PPG_ACC_dataset,\n",
        "  # according to window and overlap\n",
        "  dataset_dir = working_dir + '/PPG_ACC_dataset'\n",
        "  subj_list = [1, 2, 3, 4, 5, 6, 7]  # 1-based\n",
        "  x_data = np.empty((0, window, 4))\n",
        "  y_data = np.empty((0, 1))  # labels\n",
        "\n",
        "  # perturbations = adversarial_pattern(x_data, y_data)\n",
        "  # x_data = x_data + perturbations * 0.1\n",
        "\n",
        "\n",
        "  subj_inputs = []  # number of inputs for every subject\n",
        "  print('### creating dataset')\n",
        "  # load data from PPG_ACC_dataset and reshape for RNN\n",
        "  tot_rows = 0\n",
        "  for subject in subj_list:\n",
        "    subj_inputs.append(0)\n",
        "    for category, name in enumerate(('rest', 'squat', 'step')):\n",
        "      for record in range(0, 5):\n",
        "        acc = scipy.io.loadmat(f'{dataset_dir}/S{subject}/{name}{record + 1}_acc.mat')['ACC']\n",
        "        ppg = scipy.io.loadmat(f'{dataset_dir}/S{subject}/{name}{record + 1}_ppg.mat')['PPG'][:, 0:2]  # some PPG files have 3 columns instead of 2\n",
        "        fusion = np.hstack((acc[:, 1:], ppg[:, 1:]))  # remove x axis (time)\n",
        "        tot_rows += len(fusion)\n",
        "        clean_data(fusion)\n",
        "        # decimation (optional)\n",
        "        if decimation_factor:\n",
        "          fusion2 = np.empty((fusion.shape[0] // decimation_factor, fusion.shape[1]))\n",
        "          for col in range(0, 4):\n",
        "            #tmp = scipy.signal.decimate(fusion[:, col], decimation_factor)\n",
        "            tmp = fusion[:, col][::decimation_factor]  # simpler method\n",
        "            fusion2[:, col] = tmp[:len(fusion2)]\n",
        "          fusion = fusion2\n",
        "        # windowing\n",
        "        # compute number of windows (lazy way)\n",
        "        i = 0\n",
        "        num_w = 0\n",
        "        while i + window  <= len(fusion):\n",
        "          i += (window - overlap)\n",
        "          num_w += 1\n",
        "        # compute actual windows\n",
        "        x_data_part = np.empty((num_w, window, 4))  # preallocate\n",
        "        i = 0\n",
        "        for w in range(0, num_w):\n",
        "          x_data_part[w] = fusion[i:i + window]\n",
        "          i += (window - overlap)\n",
        "        x_data = np.vstack((x_data, x_data_part))\n",
        "        y_data = np.vstack((y_data, np.full((num_w, 1), category)))\n",
        "        subj_inputs[-1] += num_w\n",
        "\n",
        "  # normalize single windows\n",
        "  for w in x_data:\n",
        "    # remove mean value from ACC\n",
        "    w[:, 0] -= np.mean(w[:, 0])  # acc 1\n",
        "    w[:, 1] -= np.mean(w[:, 1])  # acc 2\n",
        "    w[:, 2] -= np.mean(w[:, 2])  # acc 3\n",
        "    # standardize PPG\n",
        "    w[:, 3] = StandardScaler().fit_transform(w[:, 3].reshape(-1, 1)).reshape((w.shape[0],))  # PPG\n",
        "\n",
        "  print('tot samples:', tot_rows)\n",
        "  print('x_data:', x_data.shape)\n",
        "  print('y_data:', y_data.shape)\n",
        "  print('inputs per subject:', subj_inputs)\n",
        "  if distrib_stat:\n",
        "    n = sum(subj_inputs[:distrib_stat])\n",
        "    print('class distribution (first', distrib_stat, 'subjects):', np.sum(y_data[:n] == 0), np.sum(y_data[:n] == 1), np.sum(y_data[:n] == 2))\n",
        "  return x_data, y_data, subj_inputs\n",
        "\n",
        "\n",
        "def clean_data(fusion):\n",
        "  # some tracks have isolated NaNs\n",
        "  for col in range(0, 4):\n",
        "    ids = np.where(np.isnan(fusion[:, col]))[0]\n",
        "    for row in ids:\n",
        "      fusion[row, col] = 0.5 * (fusion[row - 1, col] + fusion[row + 1, col])\n",
        "  # some PPG tracks have periodic, isolated zeros, resulting in spikes\n",
        "  for col in range(3, 4):\n",
        "    ids = np.where(fusion[:, col] == 0)[0]\n",
        "    for row in ids:\n",
        "      fusion[row, col] = 0.5 * (fusion[row - 1, col] + fusion[row + 1, col])\n",
        "  # many acc. tracks have periodic, single-point spikes of no specific values. Let's test all of them, even if it's slow\n",
        "  for col in range(0, 3):\n",
        "    for row in range(1, len(fusion) - 1):\n",
        "      if abs(fusion[row, col] - fusion[row - 1, col]) > 5000 and abs(fusion[row, col] - fusion[row + 1, col]) > 5000:\n",
        "        fusion[row, col] = 0.5 * (fusion[row - 1, col] + fusion[row + 1, col])\n",
        "\n",
        "\n",
        "def partition_data(subjects):\n",
        "  # subjects = tuple (0-based)\n",
        "  x_part = None\n",
        "  y_part = None\n",
        "  for subj in subjects:\n",
        "    skip = sum(subj_inputs[:subj])\n",
        "    num = subj_inputs[subj]\n",
        "    xx = x_data[skip : skip + num]\n",
        "    yy = y_data[skip : skip + num]\n",
        "    if x_part is None:\n",
        "      x_part = xx.copy()\n",
        "      y_part = yy.copy()\n",
        "    else:\n",
        "      x_part = np.vstack((x_part, xx))  # vstack creates a copy of the data\n",
        "      y_part = np.vstack((y_part, yy))\n",
        "  return x_part, y_part\n",
        "\n",
        "\n",
        "def oversampling(x_data, y_data, subj_inputs, num_subjects):\n",
        "  # Duplicate inputs with classes occurring less, so to have a more balanced distribution.\n",
        "  # We want to do that on a per-subject basis, so to keep subjects separate.\n",
        "  # Moreover, we do that only to the first num_subjects subjects, so to leave test subjects unaltered.\n",
        "  x_data_over = None\n",
        "  y_data_over = None\n",
        "  subj_inputs_over = []\n",
        "  skip = 0\n",
        "  for subj_num in subj_inputs[:num_subjects]:\n",
        "    x_part = x_data[skip : skip + subj_num]\n",
        "    y_part = y_data[skip : skip + subj_num]\n",
        "    occurr = (np.sum(y_part == 0), np.sum(y_part == 1), np.sum(y_part == 2))\n",
        "    assert(occurr[0] == max(occurr))\n",
        "    mul = (1, occurr[0] // occurr[1], occurr[0] // occurr[2])\n",
        "    for cl in (1, 2):\n",
        "      mask = y_part[:, 0] == cl\n",
        "      x_dup = x_part[mask].copy()\n",
        "      y_dup = y_part[mask].copy()\n",
        "      for n in range(0, mul[cl] - 1):\n",
        "        x_part = np.vstack((x_part, x_dup))\n",
        "        y_part = np.vstack((y_part, y_dup))\n",
        "    if x_data_over is None:\n",
        "      x_data_over = x_part\n",
        "      y_data_over = y_part\n",
        "    else:\n",
        "      x_data_over = np.vstack((x_data_over, x_part))\n",
        "      y_data_over = np.vstack((y_data_over, y_part))\n",
        "    subj_inputs_over.append(len(x_part))\n",
        "    skip += subj_num\n",
        "  x_data_over = np.vstack((x_data_over, x_data[skip:]))  # subjects not oversampled\n",
        "  y_data_over = np.vstack((y_data_over, y_data[skip:]))\n",
        "  subj_inputs_over.extend(subj_inputs[num_subjects:])\n",
        "  print('After oversampling', num_subjects, 'subjects:')\n",
        "  print('x_data:', x_data_over.shape)\n",
        "  print('y_data:', y_data_over.shape)\n",
        "  print('inputs per subject:', subj_inputs_over)\n",
        "  n = sum(subj_inputs_over[:num_subjects])\n",
        "  print('class distribution (first', num_subjects, 'subjects):', np.sum(y_data_over[:n] == 0), np.sum(y_data_over[:n] == 1), np.sum(y_data_over[:n] == 2))\n",
        "  return x_data_over, y_data_over, subj_inputs_over\n",
        "\n",
        "\n",
        "def create_model(window, dense1, lstm1, lstm2, lstm3 = 0, dropout = 0.2):\n",
        "  print(\"### creating model\")\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.Input(shape = (window, 4)))\n",
        "  if dense1: model.add(Dense(dense1, name = 'dense1'))\n",
        "  model.add(BatchNormalization(name = 'norm'))\n",
        "  #model.add(Dropout(dropout, name = 'drop1'))\n",
        "  model.add(LSTM(lstm1, return_sequences = bool(lstm2), name = 'lstm1'))\n",
        "  if lstm2:\n",
        "    model.add(Dropout(dropout, name = 'drop2'))\n",
        "    model.add(LSTM(lstm2, return_sequences = bool(lstm3), name = 'lstm2'))\n",
        "  if lstm3:\n",
        "    model.add(Dropout(dropout, name = 'drop3'))\n",
        "    model.add(LSTM(lstm3, name = 'lstm3'))\n",
        "  model.add(Dropout(dropout, name = 'drop4'))\n",
        "  model.add(Dense(3, name = 'dense2'))\n",
        "  model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "    optimizer = 'adam', metrics = ['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgj2Y80cIpkh",
        "outputId": "b44f030e-c979-4b1a-98f5-7780886e4668"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tf Keras: 2.11.0\n",
            "TensorFlow: 2.11.0\n",
            "GPU device: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_session(save_model = True, load_model = None, write_report = True, file_id = ''):\n",
        "\n",
        "  def write_values():\n",
        "    print(time.strftime('%H:%M:%S'), file = out_f)\n",
        "    print('window', window, 'overlap', overlap, 'decimation', decimation, file = out_f)\n",
        "    print('layers', dense1, lstm1, lstm2, lstm3, file = out_f)\n",
        "    print('oversample', oversample, file = out_f)\n",
        "    print('subj_train', permutation, file = out_f)\n",
        "    print('epochs', epochs, file = out_f)\n",
        "    if history is not None:\n",
        "      print('fit_accuracy', [round(x, 4) for x in history.history['accuracy']], file = out_f)\n",
        "    if history is not None and 'val_accuracy' in history.history:\n",
        "      print('fit_val_accuracy', [round(x, 4) for x in history.history['val_accuracy']], file = out_f)\n",
        "    print('subj_test', subjs_test, file = out_f)\n",
        "    print('test_accuracy', round(eval_metrics[1], 4), file = out_f)\n",
        "    print('permutation', perm_index + 1, file = out_f)\n",
        "    # print('iteration', iter + 1, file = out_f)\n",
        "    print('time_train', time_train, file = out_f)\n",
        "    print('time_test', time_test, file = out_f)\n",
        "    print(file = out_f)\n",
        "    out_f.flush()\n",
        "\n",
        "  if write_report:\n",
        "    output_file = time.strftime('%Y%m%d-%H%M%S') + '.txt'\n",
        "    out_f = open(working_dir + '/' + output_file, 'w')\n",
        "  # tensorboard stuff\n",
        "  #%rm -rf \"$log_dir_base\"\n",
        "  log_dir = log_dir_base + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "  for perm_index, permutation in enumerate(subjs_train_perm):\n",
        "    assert(type(permutation) == tuple)\n",
        "    assert(len(permutation) == 2)\n",
        "    assert(type(permutation[0]) == tuple)\n",
        "    assert(type(permutation[1]) == tuple)\n",
        "    assert(type(subjs_test) == tuple)\n",
        "    \n",
        "    if load_model is None:\n",
        "      model = create_model(window, dense1, lstm1, lstm2, lstm3)\n",
        "      # model training\n",
        "      x_data_train, y_data_train = partition_data(permutation[0])  # train subjects\n",
        "\n",
        "      print(\"Mean: \", np.mean(x_data_train[:,:, 3]))\n",
        "      print(\"Deviation: \", np.std(x_data_train[:,:, 3]))\n",
        "\n",
        "      x_data_val, y_data_val = partition_data(permutation[1])  # validation subjects, can be None\n",
        "      print(f'### training with {len(x_data_train)} inputs, {len(x_data_val) if x_data_val is not None else 0} validation')\n",
        "      tensorboard_callback = keras.callbacks.TensorBoard(log_dir + f'_{perm_index + 1}_{1}', histogram_freq = 1)\n",
        "      # train\n",
        "      start_time = time.monotonic()\n",
        "      history = model.fit(x_data_train, y_data_train, epochs = epochs,\n",
        "        validation_data = (x_data_val, y_data_val) if x_data_val is not None else None,\n",
        "        callbacks = [tensorboard_callback])\n",
        "      time_train = time.monotonic() - start_time\n",
        "    else:\n",
        "      # model must match with dataset parameters\n",
        "      model = keras.models.load_model(load_model)\n",
        "      history = None\n",
        "      time_train = 0\n",
        "\n",
        "    # model test\n",
        "    x_data_test, y_data_test = partition_data(subjs_test)\n",
        "    print(f'### testing with {len(x_data_test)} inputs')\n",
        "    start_time = time.monotonic()\n",
        "    eval_metrics = model.evaluate(x_data_test, y_data_test)\n",
        "    time_test = time.monotonic() - start_time\n",
        "    if write_report:\n",
        "      write_values()\n",
        "    if save_model:\n",
        "      # save in both directory and h5 formats (we had problems with both of them sometimes)\n",
        "      model_file_name = f'{working_dir}/{file_id}model_w{window:04d}_o{overlap:03d}_d{decimation:03d}_e{epochs}_t{round(eval_metrics[1] * 10000)}'\n",
        "      #model.save(model_file_name)\n",
        "      model.save(model_file_name + '.h5')\n",
        "\n",
        "  if write_report:\n",
        "    out_f.close()"
      ],
      "metadata": {
        "id": "6spkP2K0DUBA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset with given window and overlap\n",
        "if __name__ == '__main__':\n",
        "  window = 1200\n",
        "  overlap = window // 2\n",
        "  oversample = False\n",
        "  decimation = 0\n",
        "  if decimation:\n",
        "    window //= decimation\n",
        "    overlap //= decimation\n",
        "  x_data, y_data, subj_inputs = create_dataset(window, overlap, decimation, distrib_stat = 5)\n",
        "  if oversample:\n",
        "    x_data, y_data, subj_inputs = oversampling(x_data, y_data, subj_inputs, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_qFN2GIH7M3",
        "outputId": "bbd3735f-f0b3-425f-81d9-25c84a1adaa8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### creating dataset\n",
            "tot samples: 6880418\n",
            "x_data: (11310, 1200, 4)\n",
            "y_data: (11310, 1)\n",
            "inputs per subject: [2663, 2364, 1193, 1205, 1264, 1288, 1333]\n",
            "class distribution (first 5 subjects): 6871 773 1045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create and train model\n",
        "if __name__ == '__main__':\n",
        "  dense1 = 8\n",
        "  lstm1 = 8\n",
        "  lstm2 = 8\n",
        "  lstm3 = 8\n",
        "  # subjs_train_perm = ( ((0, 1, 2, 3, 4), ()), )  # final model\n",
        "  subjs_train_perm = ( ((0, 1, 2), ()), )  # final model\n",
        "  #subjs_train_perm = ( ((0, 1, 2, 3), (4,)), ((0, 1, 2, 4), (3,)), ((0, 1, 3, 4), (2,)), ((0, 2, 3, 4), (1,)), ((1, 2, 3, 4), (0,)) )  # cross-validation\n",
        "  subjs_test = (5, 6)\n",
        "  epochs = 100\n",
        "  iterations = 1  # repeat every test to cope with randomness\n",
        "  train_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_Z_5vZJdCMQ",
        "outputId": "ff91c930-ce5b-4104-89fe-8c95339b835b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### creating model\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense1 (Dense)              (None, 1200, 8)           40        \n",
            "                                                                 \n",
            " norm (BatchNormalization)   (None, 1200, 8)           32        \n",
            "                                                                 \n",
            " lstm1 (LSTM)                (None, 1200, 8)           544       \n",
            "                                                                 \n",
            " drop2 (Dropout)             (None, 1200, 8)           0         \n",
            "                                                                 \n",
            " lstm2 (LSTM)                (None, 1200, 8)           544       \n",
            "                                                                 \n",
            " drop3 (Dropout)             (None, 1200, 8)           0         \n",
            "                                                                 \n",
            " lstm3 (LSTM)                (None, 8)                 544       \n",
            "                                                                 \n",
            " drop4 (Dropout)             (None, 8)                 0         \n",
            "                                                                 \n",
            " dense2 (Dense)              (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,731\n",
            "Trainable params: 1,715\n",
            "Non-trainable params: 16\n",
            "_________________________________________________________________\n",
            "Mean:  -2.8729382812233674e-16\n",
            "Deviation:  1.0000000000000002\n",
            "### training with 6220 inputs, 0 validation\n",
            "Epoch 1/100\n",
            "195/195 [==============================] - 30s 91ms/step - loss: 0.5973 - accuracy: 0.8177\n",
            "Epoch 2/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.4029 - accuracy: 0.8473\n",
            "Epoch 3/100\n",
            "195/195 [==============================] - 16s 83ms/step - loss: 0.3861 - accuracy: 0.8568\n",
            "Epoch 4/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3682 - accuracy: 0.8674\n",
            "Epoch 5/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3675 - accuracy: 0.8699\n",
            "Epoch 6/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3571 - accuracy: 0.8691\n",
            "Epoch 7/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3625 - accuracy: 0.8609\n",
            "Epoch 8/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.3460 - accuracy: 0.8748\n",
            "Epoch 9/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.3561 - accuracy: 0.8743\n",
            "Epoch 10/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3508 - accuracy: 0.8746\n",
            "Epoch 11/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3454 - accuracy: 0.8706\n",
            "Epoch 12/100\n",
            "195/195 [==============================] - 16s 83ms/step - loss: 0.3521 - accuracy: 0.8659\n",
            "Epoch 13/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3384 - accuracy: 0.8752\n",
            "Epoch 14/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3334 - accuracy: 0.8722\n",
            "Epoch 15/100\n",
            "195/195 [==============================] - 16s 82ms/step - loss: 0.3746 - accuracy: 0.8521\n",
            "Epoch 16/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3653 - accuracy: 0.8535\n",
            "Epoch 17/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3531 - accuracy: 0.8624\n",
            "Epoch 18/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3588 - accuracy: 0.8693\n",
            "Epoch 19/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3755 - accuracy: 0.8576\n",
            "Epoch 20/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3622 - accuracy: 0.8608\n",
            "Epoch 21/100\n",
            "195/195 [==============================] - 16s 83ms/step - loss: 0.3593 - accuracy: 0.8606\n",
            "Epoch 22/100\n",
            "195/195 [==============================] - 17s 85ms/step - loss: 0.3554 - accuracy: 0.8621\n",
            "Epoch 23/100\n",
            "195/195 [==============================] - 15s 79ms/step - loss: 0.3449 - accuracy: 0.8690\n",
            "Epoch 24/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3394 - accuracy: 0.8699\n",
            "Epoch 25/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3459 - accuracy: 0.8677\n",
            "Epoch 26/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3353 - accuracy: 0.8722\n",
            "Epoch 27/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3475 - accuracy: 0.8659\n",
            "Epoch 28/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3406 - accuracy: 0.8691\n",
            "Epoch 29/100\n",
            "195/195 [==============================] - 16s 81ms/step - loss: 0.3372 - accuracy: 0.8738\n",
            "Epoch 30/100\n",
            "195/195 [==============================] - 15s 79ms/step - loss: 0.3356 - accuracy: 0.8738\n",
            "Epoch 31/100\n",
            "195/195 [==============================] - 16s 82ms/step - loss: 0.3379 - accuracy: 0.8703\n",
            "Epoch 32/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3298 - accuracy: 0.8759\n",
            "Epoch 33/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3327 - accuracy: 0.8765\n",
            "Epoch 34/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3241 - accuracy: 0.8831\n",
            "Epoch 35/100\n",
            "195/195 [==============================] - 15s 79ms/step - loss: 0.3244 - accuracy: 0.8804\n",
            "Epoch 36/100\n",
            "195/195 [==============================] - 16s 83ms/step - loss: 0.3465 - accuracy: 0.8619\n",
            "Epoch 37/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3579 - accuracy: 0.8669\n",
            "Epoch 38/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3514 - accuracy: 0.8677\n",
            "Epoch 39/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3447 - accuracy: 0.8699\n",
            "Epoch 40/100\n",
            "195/195 [==============================] - 16s 83ms/step - loss: 0.3452 - accuracy: 0.8744\n",
            "Epoch 41/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3373 - accuracy: 0.8719\n",
            "Epoch 42/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.3336 - accuracy: 0.8752\n",
            "Epoch 43/100\n",
            "195/195 [==============================] - 16s 81ms/step - loss: 0.3375 - accuracy: 0.8701\n",
            "Epoch 44/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3426 - accuracy: 0.8672\n",
            "Epoch 45/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3265 - accuracy: 0.8799\n",
            "Epoch 46/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3310 - accuracy: 0.8751\n",
            "Epoch 47/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3251 - accuracy: 0.8822\n",
            "Epoch 48/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3178 - accuracy: 0.8770\n",
            "Epoch 49/100\n",
            "195/195 [==============================] - 17s 86ms/step - loss: 0.3150 - accuracy: 0.8770\n",
            "Epoch 50/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.3347 - accuracy: 0.8765\n",
            "Epoch 51/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.3306 - accuracy: 0.8727\n",
            "Epoch 52/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3265 - accuracy: 0.8772\n",
            "Epoch 53/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3572 - accuracy: 0.8732\n",
            "Epoch 54/100\n",
            "195/195 [==============================] - 15s 79ms/step - loss: 0.3357 - accuracy: 0.8797\n",
            "Epoch 55/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3189 - accuracy: 0.8796\n",
            "Epoch 56/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3391 - accuracy: 0.8732\n",
            "Epoch 57/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3312 - accuracy: 0.8717\n",
            "Epoch 58/100\n",
            "195/195 [==============================] - 17s 88ms/step - loss: 0.3262 - accuracy: 0.8768\n",
            "Epoch 59/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3256 - accuracy: 0.8815\n",
            "Epoch 60/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3326 - accuracy: 0.8719\n",
            "Epoch 61/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3235 - accuracy: 0.8794\n",
            "Epoch 62/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3190 - accuracy: 0.8785\n",
            "Epoch 63/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3321 - accuracy: 0.8812\n",
            "Epoch 64/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.3205 - accuracy: 0.8839\n",
            "Epoch 65/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.3168 - accuracy: 0.8854\n",
            "Epoch 66/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3135 - accuracy: 0.8863\n",
            "Epoch 67/100\n",
            "195/195 [==============================] - 17s 86ms/step - loss: 0.3545 - accuracy: 0.8675\n",
            "Epoch 68/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3374 - accuracy: 0.8812\n",
            "Epoch 69/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3407 - accuracy: 0.8826\n",
            "Epoch 70/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3141 - accuracy: 0.8873\n",
            "Epoch 71/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.3291 - accuracy: 0.8711\n",
            "Epoch 72/100\n",
            "195/195 [==============================] - 16s 82ms/step - loss: 0.3226 - accuracy: 0.8706\n",
            "Epoch 73/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3137 - accuracy: 0.8801\n",
            "Epoch 74/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3258 - accuracy: 0.8756\n",
            "Epoch 75/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3255 - accuracy: 0.8764\n",
            "Epoch 76/100\n",
            "195/195 [==============================] - 16s 84ms/step - loss: 0.3303 - accuracy: 0.8777\n",
            "Epoch 77/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3256 - accuracy: 0.8777\n",
            "Epoch 78/100\n",
            "195/195 [==============================] - 16s 81ms/step - loss: 0.3142 - accuracy: 0.8781\n",
            "Epoch 79/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.3115 - accuracy: 0.8818\n",
            "Epoch 80/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3305 - accuracy: 0.8804\n",
            "Epoch 81/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3336 - accuracy: 0.8751\n",
            "Epoch 82/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3370 - accuracy: 0.8717\n",
            "Epoch 83/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3320 - accuracy: 0.8691\n",
            "Epoch 84/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3263 - accuracy: 0.8738\n",
            "Epoch 85/100\n",
            "195/195 [==============================] - 17s 89ms/step - loss: 0.3174 - accuracy: 0.8794\n",
            "Epoch 86/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3157 - accuracy: 0.8812\n",
            "Epoch 87/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3141 - accuracy: 0.8924\n",
            "Epoch 88/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3212 - accuracy: 0.8833\n",
            "Epoch 89/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3026 - accuracy: 0.8926\n",
            "Epoch 90/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3012 - accuracy: 0.8924\n",
            "Epoch 91/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3021 - accuracy: 0.8915\n",
            "Epoch 92/100\n",
            "195/195 [==============================] - 16s 82ms/step - loss: 0.2995 - accuracy: 0.8942\n",
            "Epoch 93/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.2999 - accuracy: 0.8971\n",
            "Epoch 94/100\n",
            "195/195 [==============================] - 17s 85ms/step - loss: 0.3797 - accuracy: 0.8482\n",
            "Epoch 95/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3796 - accuracy: 0.8498\n",
            "Epoch 96/100\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3701 - accuracy: 0.8529\n",
            "Epoch 97/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3573 - accuracy: 0.8574\n",
            "Epoch 98/100\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.3477 - accuracy: 0.8648\n",
            "Epoch 99/100\n",
            "195/195 [==============================] - 16s 81ms/step - loss: 0.3285 - accuracy: 0.8738\n",
            "Epoch 100/100\n",
            "195/195 [==============================] - 16s 80ms/step - loss: 0.3247 - accuracy: 0.8773\n",
            "### testing with 2621 inputs\n",
            "82/82 [==============================] - 4s 34ms/step - loss: 0.3383 - accuracy: 0.8619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model test only\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/dataset/epo5_eps0.5.h5')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  x_data_test, y_data_test = partition_data(subjs_test)\n",
        "  print(f'### testing with {len(x_data_test)} inputs')\n",
        "  eval_metrics = model.evaluate(x_data_test, y_data_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbVGYgfJDw-Y",
        "outputId": "db97170c-1b5e-4162-d774-f50a5e2bf5b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### testing with 2621 inputs\n",
            "82/82 [==============================] - 4s 35ms/step - loss: 0.4578 - accuracy: 0.7959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create confusion matrix\n",
        "if __name__ == '__main__':\n",
        "  import pandas as pd\n",
        "  import seaborn\n",
        "  # y_pred = pretrained_model.predict_classes(x_data_test)\n",
        "  y_prob = model.predict(x_data_test) \n",
        "  y_pred = y_prob.argmax(axis=-1)\n",
        "  con_mat = tf.math.confusion_matrix(labels = y_data_test, predictions = y_pred).numpy()\n",
        "  con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis = 1)[:, np.newaxis], decimals = 2)\n",
        "  classes = ['resting', 'squat', 'stepper']\n",
        "  con_mat_df = pd.DataFrame(con_mat_norm, index = classes, columns = classes)\n",
        "  #con_mat_df['squat']['stepper']=0.11  # fix sum not 1 due to rounding\n",
        "  #print(con_mat_df)\n",
        "  figure = plt.figure(figsize = (5, 5))\n",
        "  seaborn.heatmap(con_mat_df, annot = True, cmap = plt.cm.Blues)\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(working_dir + '/confusion_matrix.eps', format='eps')\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "OYmYCKzUEdzm",
        "outputId": "2ab1bd11-996a-45b9-eb13-0e5b8517c911"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82/82 [==============================] - 4s 34ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFgCAYAAAD+RWGAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqN0lEQVR4nO3deXxVxf3/8dcnCZvKvgQkIKi4oNYNFbUq4oZLRW3dar/WqkVbd6v9uVRrad21WhUXXKq1LnUvCkWrdS8qSFVARRFBQJbIIiCyJZ/fH+ck3ASykDC5mZv3k8d95J5z586Zex6XTz6ZMzPH3B0REdnw8rLdABGRXKUAKyISiAKsiEggCrAiIoEowIqIBKIAKyISiAKsiAhgZg+Y2Twzm1jF62Zmt5nZFDP7yMx2qalOBVgRkcSDwKBqXj8U6JM+hgB31VShAqyICODubwALqikyGPibJ94B2plZt+rqLNiQDdyQWu18tqaYbQALx96R7SaIlGtZgNX1vfWNCcs/GHYGSeZZZri7D1+PKroDMzK2Z6b7Zlf1hkYbYEVENqQ0mK5PQK03BVgRiYNlvUdzFtAjY7so3VelrLdYRKRWzOr3qL8RwMnpaIL+wLfuXmX3ACiDFZFYBM5gzewxYADQycxmAr8HmgG4+93AKOAwYAqwDPhFTXUqwIpIHDZMFloldz+xhtcdOGt96lSAFZE4ZL8Pdr3F12IRkUgogxWROATuIghBAVZE4hBhF4ECrIjEQRmsiEggymBFRAJRBisiEkiEGWx8LRYRiYQyWBGJg7oIREQCibCLQAFWROKgACsiEkieughERMKIMIONr8UiIpFQBisicdAoAhGRQCLsIlCAFZE4KIMVEQlEGayISCDKYEVEAokwg42vxSIikVAGKyJxUBeBiEggEXYRKMCKSByUwYqIBKIMVkQkkAgDbHwtFhGJhDJYEYmD+mBFRAKJsItAAVZE4qAMtiIzmwB4pd3fAuOAP7n7/JDHF5Ecogx2Lf8CSoBH0+0TgI2AOcCDwI8CH19EcoUy2LUc6O67ZGxPMLPx7r6Lmf0s8LFFRLIqdM6db2a7l22Y2W5Afrq5OvCxRSSHmFm9HtkQOoM9HXjAzDYBDFgMnG5mGwPXBj62iOSQbAXJ+ggaYN19LLCDmbVNt7/NePmJkMcWkRwTX3wNPoqgBfBjoBdQUPYbyN2HhjyuiOQeZbBr+yfJsKz3gRWBjyUiOUwBdm1F7j4o8DFEpAmIMcCGHkXwXzPbIfAxREQapdAB9ofA+2Y22cw+MrMJZvZR4GMGd/fvT2L6K9cy7snLst2U6L395hscefghHDHoIO6/d3i2mxOtpnAeYxymFTrAHgr0AQ4mmbV1BDkwe+vh599h8FnDst2M6JWUlHDN1UO58+77eHbESEaPeoEvpkzJdrOi02TOo9XzkQVBAqyZtUmfLqniEbW3x3/Bgm+XZbsZ0Zs44SN69NiMoh49aNa8OYMOO5zXXn0l282KTlM5jzFmsKEucj1Kkq2+T7LYS+anc2DzQMeViMybO5eu3bqWb3cpLGTCR9H3IDW4pnIeY7zIFSTAuvsR6c/e6/M+MxsCDAEoKBpAQaftArRORGIUY4AN2gdrZmv9nbKufWXcfbi793P3fgquua9LYSFzZs8p3543dy6FhYVZbFGcdB4br1B9sC3NrAPQyczam1mH9NEL6B7imBKf7bbfga++msbMmTNYtXIlo0eNZL/9B2a7WdFpKudRfbBrnAGcD2xK0g9b9ukWA3cEOmaDeejaU9hn1z50arcJU0b/kT/ePYqHnhuT7WZFp6CggEsvv5JfDTmd0tISjjr6x2y5ZZ9sNys6TeY8xtdDgLlXvuHABqzc7Bx3v70u722189nhGtaELBwb/e8zySEtC+oeJjud8ni9YsI3D57Q4CE69DjYOWbWGsDMfmdmz5jZLjW9SUSkshi7CEIH2CvcfYmZ/RA4ELgfuCvwMUUkBynArq0k/Xk4MNzdRwLNAx9TRHJR4JlcZjYondY/xcwuWcfrPc3sVTP7Xzr1/7Ca6gwdYGeZ2T3A8cCodH3Y+G4NKSI5zczygWEk0/v7AieaWd9KxX4HPOHuO5PcwPXOmuoNHeyOA14EDnH3RUAH4OLAxxSRHBS4i2B3YIq7T3X3lcDjwOBKZRwoWwagLfB1TZUGDbDuvgyYR7KqFiQ3Ovw85DFFJDfVN8Ca2RAzG5fxGJJRfXdgRsb2TNYes38V8DMzmwmMAs6pqc2hbxnze6AfsDXwV6AZ8Hdg75DHFZHcU98LVe4+HKjPWo4nAg+6+81mtifwsJlt7+6lVb0h9B0NjgZ2BsYDuPvXZcO2RETWR+CRALOAHhnbRem+TKcBgwDcfYyZtQQ6kfyVvk6h+2BXejKTwQEsuV23iMj6CzuKYCzQx8x6m1lzkotYIyqV+Qo4AMDMtgVaAsXVVRoswFry6+aFdBRBOzP7JfAycG+oY4qI1IW7rwbOJrko/wnJaIFJZjbUzI5Mi/0G+KWZfQg8BpziNUyFDdZF4O5uZscCF5KsQbA1cKW7/zvUMUUkd4WeLODuo0guXmXuuzLj+ces5/Wj0H2w44FF7q6hWSJSL9majVUfoQPsHsBJZjYd+K5sp7v/IPBxRSTHKMCu7ZDA9YtIUxFffA0bYN19esj6RaTpiDGD1boAIiKBhO4iEBHZIGLMYBVgRSQKCrAiIoEowIqIhBJffFWAFZE4KIMVEQkkxgCrYVoiIoEogxWRKESYwCrAikgcYuwiUIAVkShEGF8VYEUkDspgRUQCiTC+ahSBiEgoymBFJAp5efGlsAqwIhKFGLsIFGBFJAq6yCUiEkiE8VUBVkTioAxWRCSQGAOshmmJiASiDFZEohBhAqsAKyJxiLGLQAFWRKIQYXxVgBWROCiDFREJJML4qlEEIiKhKIMVkSioi0BEJJAI46sCrIjEQRmsiEggEcbXxhtgdz7xuGw3ISfMW7wi203ICXO/XZ7tJuSE3Xq3rfN7Y8xgNYpARCSQRpvBiohkijCBVYAVkTjE2EWgACsiUYgwvirAikgclMGKiASiACsiEkiE8VXDtEREQlEGKyJRUBeBiEggEcZXBVgRiYMyWBGRQCKMrwqwIhKHvAgjrEYRiIgEogxWRKIQYQKrACsicYjxIpe6CEQkCnlWv0dNzGyQmU02sylmdkkVZY4zs4/NbJKZPVpTncpgRSQKITNYM8sHhgEHATOBsWY2wt0/zijTB7gU2NvdF5pZl5rqVQYrIlEwq9+jBrsDU9x9qruvBB4HBlcq80tgmLsvBHD3eTVVqgArIlGw+v4zG2Jm4zIeQzKq7w7MyNieme7LtBWwlZm9bWbvmNmgmtqsLgIRaRLcfTgwvB5VFAB9gAFAEfCGme3g7ouqe4OISKNXmwtV9TAL6JGxXZTuyzQTeNfdVwFfmtlnJAF3bFWVqotARKJgZvV61GAs0MfMeptZc+AEYESlMs+RZK+YWSeSLoOp1VWqDFZEohByGKy7rzazs4EXgXzgAXefZGZDgXHuPiJ97WAz+xgoAS529/nV1asAKyJRCL0WgbuPAkZV2ndlxnMHLkwftaIAKyJRiHAil/pgRURCqTKDNbPbAa/qdXc/N0iLRETWIca1CKrrIhjXYK0QEalBhPG16gDr7g9lbpvZRu6+LHyTRETWlpMLbpvZnumwhE/T7R3N7M7gLRMRyWD1fGRDbS5y3QocAswHcPcPgX0DtklEZC2BJxoEUatRBO4+o9KukgBtERHJKbUZBzvDzPYC3MyaAecBn4RtlohIRYHXIgiiNgH2TOAvJEt3fU0yXeyskI0SEaks14ZpAeDu3wAnNUBbRESqFGF8rdUogs3N7HkzKzazeWb2TzPbvCEaJyJSJlcvcj0KPAF0AzYFngQeC9koEZHKQt/0MEiba1FmI3d/2N1Xp4+/Ay1rU7mZPVybfSIiNYkxg61uLYIO6dN/pbewfZxkbYLjqbSkVzW2q1RnPrBrHdopIhKd6i5yvU8SUMtC/xkZrznJ7WvXycwuBS4DWpnZ4rLdwErqd08cEWmiIrzGVe1aBL3rWqm7Xwtca2bXunuVgVhEpLZiXIugVgtum9n2QF8y+l7d/W81vc/dLzWz9iQ3Bst87xvr31QRacoijK81B1gz+z3Jjb76kvS9Hgq8BdQYYM3sdJKZX0XAB0B/YAwwsK4NFpGmKcaJBrUZRfAT4ABgjrv/AtgRaFvL+s8DdgOmu/v+wM7Aojq0U0SaOLP6PbKhNl0E37t7qZmtNrM2wDwq3j+8OsvdfXk6TKKFu39qZlvXvbkNZ4/e7Tn/gC3IN+P5j+bw8LuV17uBgVt34rS9N8OBKfO+46oXPgWgsHULLh20FV3atMDd+c1TE5mzeEUDf4LGYeyYt7jz1uspLSnl0COP4YSTT6vw+kf/G8ddt97A1C8+5/Kh17PvwIPLX7v0/DP5ZNIEtv/Bzvzp5jsauumNyofjxvDwXTdTWlrKgEGDOfL4n1d4fdTTj/DaiyPIz8undbt2DLngCjoVduObubO5ZehvcS+lZPVqDh58HAcc/uMsfYqmpzYBdpyZtQPuJRlZsJTkz/zamJm+9zng32a2EJi+/s1sWHkGFx24Jec9MYF5S1Zw/8k78+aU+Uybv2a98aL2LTm5f0/OfORDlqxYTfuNmpW/dsXhW/PQmK8YO30RrZrlUVrljXdyW0lJCbfffA3X/2U4nboUcvapJ7LnPgPYrPcW5WW6dO3GxVf8iScfeXCt9x970imsWL6ckc891YCtbnxKS0p4aNgNXHLNHXTo1IUrz/05u/bfh+6brZlQ2WvLrfnj4Q/RomVLXn7hKR67/3bOuewa2nXoxFW33E+z5s1Z/v0yLjnjRHbpvy/tO3bO4ieqm5y8yOXuv06f3m1mo4E27v5RbSp396PTp1eZ2askXQuj69TSBtS3W2tmLvqer79dDsDLnxSzz5YdKwTYI3/Qjaf/9zVLVqwGYOGyVQD06rgR+XnG2OmLAPh+VWnDNr4RmfzxRDYt6km37kUADDhwEP9949UKAbZrt+4AWN7avVW77NafD8ePbZjGNmJfTJ5EYbciuqTnqv9+B/P+mDcqBNi+O/Yrf77lNjvw9n+S/2YFzdb84l+1aiXu8X4fI4yv1U402KW619x9fE2Vm1nPjM0v059dga9q3cIs6LxJC+YuWfMnffGSFfTdtHWFMj07tALg7p/uSF6ecf/b03n3y4X0bN+KpStWc81Rfdm0bUvGTl/IXa9/2SSz2G+K59K5S2H5dqcuhXw6aUIWWxSnhfOL6dB5zXns0KkLX0yeVGX5118cwY799izfnl88l5uuuIC5s2dw4mnnRpm9QpwXuarLYG+u5jWndiMBRrJmskJLoDcwmUozvMqY2RBgCMDmx/yGwj2OrMUhsiM/z+jRvhVnPf4RXVq34M4Td+T//jqO/Dxjx6K2nPLgeOYuXs7QI7flsO278sKEOdlusjQBb73yL6Z+/gm/u+Hu8n0dOxdy7d2PsnB+Mbf84WJ232cgbdt3zGIr66ZWdwdoZKqbaLB/fSt39x0yt9Os+NdVFMfdh5PO9NrrhjeylvMVL11BYesW5dudW7egeMnKCmXmLVnBx18voaTUmf3tcmYsXEaP9q2Yt2QFn89bWt698Obn89lu0za80AQTt06dCymeN7d8+5t5c+nUuUsWWxSn9h07s6B4zXlc8M28dWahE8e/x4jH/8rlN95Ns+bN11lPUa8tmDzxA3bf54CgbQ4hxgy2QX8ppN0KezTkMevik9lLKGrfim5tW1KQZxy4bWfemjK/Qpk3Pp/Pzj3bAdC2VQE92m/ErEXL+WTOEjZpUUC7Vknf166btePL+d819EdoFLbedjtmzZjO7K9nsmrVKl57eTR77jMg282KzuZb92XO1zOYN2cWq1et4p3XX2KX/vtUKDNtymQeuP1aLrzqJtq261C+f37xXFauSH7Zf7dkMZ9N+oBuRZs1aPs3lBhX06rVTK66MrMLMzbzgF1I7orQqJU4/PnlKdxy7Pbkm/HChDl8OX8Zp/9wMz6ds4S3pizg3S8Xskev9jxy6q6UOgx7bSqLlycXvO54dSq3Hb8DZsanc5Yw4sOm2T2QX1DA2b+5jEvP/xWlpSUccsRR9Np8Sx4cPoyttu3LXvvsz+SPJ3LVJeezdMli3nnrdf52313c9+izAFxw5s+ZMX0a3y9bxolHHsiFl/2B3frvneVP1fDy8wv4+a8v5obLz6W0tJT9Dv4RRb224Km/3UPvPtuy65778th9t7H8+++57epkZnrHzl35zR9u5usZ03h0+F8wA3c47Mc/o0fvLbP8iZoOcw/3l3g6C6zMamAa8LS7L6/pvdnsIsglj5/e6P9giMLcb2v8ykot7Na7bZ1zyQtHfFqvmPDnI7dp8Dy2NlNljeSWMZu7+9B0ZEBXd3+vpve6+x82QBtFRKLsg61NF8GdQCnJqIGhwBLgaZIpsNUys+dJRhGsk7s33mECItKo5OpdZfdw913M7H8A7r7QzNa+RLluU0nGvf493T4RmEsys0tEpNYiTGBrFWBXpXcicAAz60yS0dbG3u7eL2P7eTMb5+4XrGc7RaSJi3GqbG2Gad0GPAt0MbOrSZYqvKaW9W+ceQfa9PnG691KEZEI1WYtgkfM7H2SJQsNOMrdP6ll/ecDr5nZ1HS7F+lMLRGR9ZFTM7nKpKMGlgHPZ+5z99qsJ9AG2J5kiuyRwF7AN3Vrqog0ZRH2ENSqD3a91hOo5Ap3f9LMWpOMQrgJuIsIZnOJSOOSk32w7r6Du/8g/dkH2J3arwdbkv48HLjX3UcCtR2BICJSLlfvaFCBu483s9pmoLPM7B7gIOB6M2tBnF0pIpJlOTkOtp7rCRwHDAJucvdFZtYNuHi9WykiEqHaZLCZK02vJumTfbo2lbv7MuCZjO3ZwOz1aaCICMTZB1ttgE0nGLR294saqD0iIusUYXyt9pYxBe6+2sya3vpwItLo5Fof7Hsk/a0fmNkI4EmgfOVod3+mqjeKiGxoRnwRtjZ9sC2B+STjWMvGwzoZfasiIqHlWgbbJR1BMJE1gbWMFsMWkQaVawE2H9gE1pmXK8CKiNSgugA7292HNlhLRESqkWt3NIjv04hIzsq1LoL4bpwuIjkrwgS26gDr7gsasiEiItWJcSaXFl4RkSjkWf0eNTGzQWY22cymmNkl1ZT7sZm5mfWrqkx5m9fvI4qI5J50WYBhwKFAX+BEM+u7jnKtgfOAd2tTrwKsiEQh8HqwuwNT3H2qu68EHgcGr6PcH4HrgeW1abMCrIhEIQ+r18PMhpjZuIxH5v0BuwMzMrZnpvvKmdkuQI/0xgG1st4LbouIZEN9r3G5+3BgeN2ObXnAn4FT1ud9CrAiEoXA42BnAT0ytovSfWVak9zA9bV0wkNXYISZHenu46qqVAFWRKIQeJjWWKCPmfUmCawnAD8te9HdvwU6lW2b2WvARdUFV1CAFZFIhIyv6drXZwMvkqzD8oC7TzKzocA4dx9Rl3oVYEVEAHcfBYyqtO/KKsoOqE2dCrAiEoUYZ3IpwIpIFCKMrwqwIhKHGAftK8CKSBRybT1YEZFGI77wGmfWLSISBWWwIhIFjSIQEQkkvvCqACsikYgwgVWAFZE4aBSBiEggMV6Rj7HNIiJRUAYrIlFQF4GISCDxhVcFWBGJhDLYDahZs/xsNyEndNi4ebabkBPGzViQ7SbkhN1oW+f3xnjBqNEGWBGRTMpgRUQCiS+8xpl1i4hEQRmsiEQhwh4CBVgRiUNehJ0ECrAiEgVlsCIigZgyWBGRMGLMYDWKQEQkEGWwIhIFXeQSEQkkxi4CBVgRiYICrIhIIBpFICISSF588VUBVkTiEGMGq2FaIiKBKIMVkSjoIpeISCAxdhEowIpIFHSRS0QkEGWwIiKBxNgHq1EEIiKBKIMVkShEmMAqwIpIHPIi7CNQgBWRKMQXXhVgRSQWEUZYBVgRiUKMw7Q0ikBEJBBlsCIShQivcSnAikgcIoyvCrAiEokII6wCrIhEIcaLXAqwIhKFGPtgg40iMLN8M/s0VP0i0rRYPR/ZECzAunsJMNnMeoY6hohIYxa6i6A9MMnM3gO+K9vp7kcGPq6I5JoIuwhCB9grAtcvIk1E6ItcZjYI+AuQD9zn7tdVev1C4HRgNVAMnOru06urM+hMLnd/HZgGNEufjwXGhzymiOQms/o9qq/b8oFhwKFAX+BEM+tbqdj/gH7u/gPgKeCGmtocNMCa2S/ThtyT7uoOPBfymCKSmwJf5NodmOLuU919JfA4MDizgLu/6u7L0s13gKKaKg29FsFZwN7AYgB3/xzoEviYIpKL6hlhzWyImY3LeAzJqL07MCNje2a6ryqnAf+qqcmh+2BXuPtKS/NzMysAPPAxRUTW4u7DgeH1rcfMfgb0A/arqWzoAPu6mV0GtDKzg4BfA88HPqaI5KDAF7lmAT0ytovSfRXbYHYgcDmwn7uvqKnS0F0El5BcbZsAnAGMAn4X+JgikoNCXuQiuQDfx8x6m1lz4ARgRMXj284k15OOdPd5tWlz0AzW3UvN7CHgXZKugcnuri4CEVlvIfNXd19tZmcDL5IM03rA3SeZ2VBgnLuPAG4ENgGeTLs9v6ppTH/QAGtmhwN3A1+QnJ/eZnaGu9fYOSwiUkHgiQbuPorkr+zMfVdmPD9wfesM3Qd7M7C/u08BMLMtgJHU4upbtu2+WTvOGbA5eXkwcuJcHh27VncM+2/VkVP698SBL4q/44//+qz8tY2a5/PQyTvz1hcL+MurUxuw5Y3Lf996k5uuv5qS0lKOOuYn/OK0IRVeX7lyJVde/v/45ONJtG3bjutu/DObdi/inTFvc/utN7Nq1SqaNWvGeRf+lt336J+lT5F9kz94lxf+egelpSXsdsDhDDjqpAqvv/vSPxnz4nPk5eXRvGUrjj7jIgqLejFjyic8e89NQPIn5IHHnsJ2u++ThU9Qf1pNa21LyoJraiqwJPAx6y3P4PyBm/ObZyZRvGQl9/x0R97+YgHTF3xfXqZ7u5actFsRZ/3jI5auKKFdq2YV6jhtr558NGtxQze9USkpKeG6a4Zy5/AHKCws5P9OPJb9Bgxk8y22LC/z3DNP0aZNG/458iVe/NdIbrv1Zq678RbatWvPrbffRecuhUz5/DPO/tXpjH75jSx+muwpLS1hxP1/4bTf3USbjp0ZdumZbNtvbwqLepWX2fGHB7LHwcmwzY/Hvc3Ih4Zx6uU3UtijN2dddw/5+QUsXjif2y4+jW123ZP8/PgW0tNqWmsbZ2ajzOwUM/s5yQiCsWZ2jJkdE/jYdbZt19bMWrSc2d+uYHWp85/Jxfxwiw4Vyvxoh0Ke/XAOS1eUALDo+1Xlr23VZWPab9SMsdMXNWSzG51JEz+iR8+eFBX1oFmz5hw86DBee/WVCmVef+0VjjjyKAAOOOgQ3nt3DO7ONtv2pXOXQgC22LIPK5avYOXKlQ39ERqFGVM+pWPX7nQo3JSCgmbsuNdAPhn7doUyLTfauPz5yuXLKRsa2bxFy/JgunrVyvL90jBC/xprCcxlzXixYqAV8COSv1ieCXz8Oum0SXPmLVnzn7l46Uq27dq6Qpmidq0AuOP4Hcgz48ExX/He9EUY8Ot9e3P16M/YtWe7Bmx14zNv7lwKC7uVbxcWdmXihA8rlCmeO6+8TEFBAZts0ppFixbRvn378jKv/PtFttm2L82bN2+YhjcyixcU07Zj5/LtNh07M+Pzj9cqN2b0s7w18klKVq/i9CtvKd//1ecf8/RdN7CoeA7HnXN5lNkrRLnWS/BRBL9Yn/LpzIohAH2OvZhuew6u4R3Zk59nFLVrxXlPTqTzJs25/bgd+MXD/+OgbTrz7rSFFC9tmtnWhvbFlM+57dabGXbP/dluSqO356Cj2XPQ0Xzw1sv85+mHOe7sSwHo2acvF/z5QebNnM6Tw65lq512p1nzFllubR1EGGFDr0WwuZk9b2bFZjbPzP5pZptXVd7dh7t7P3fvl83g+s3SlXRpvSZb6rxJc75ZWnFMcfHSlbz9xQJKSp05i1cwY+H3FLVrxXbd2nD0jt14/NRd+dW+vThk284M+eFmDf0RGoUuhYXMnTu7fHvu3Dnlf/aX6VzYpbzM6tWrWbp0Ce3atUvKz5nDRReczdCrr6dHj6a7rHCbDp35dn5x+fbi+cW07dC5yvI/2GsgH499a639XYo2o3nLVsyd8WWQdoZm9fyXDaH7YB8FngC6AZsCTwKPBT5mvX06ZwlF7VvRtU0LCvKMgVt35u2pCyqUeWvKfHbq0RaAti0L6NG+FV9/u5w/jf6M4+4fxwkPvM9db0zjxU+KGf5WtSua5ay+2+3AjOnTmTVzJqtWreSl0aPYb8DACmX2GzCQF0Y8ByRdAbvt3h8zY8nixZx39hmcc95v2GnnXbLQ+sajaIut+Wb2TBbMm83q1av48L//Ydt+e1Uo883smeXPJ49/h07dkmn0C+bNpqRkNQALi+dQ/PVXtO/cteEavwEFnmgQROjOmI3c/eGM7b+b2cWBj1lvJQ63/mcqNx2zHXkGoybNY9r87zl1z558Oncp/526gPemL2K3zdrx0Mk7U+rOXW9MY/Hy1dlueqNSUFDAby+7grN/dRolJaUMPurHbLFlH+4adht9+27PfvsPZPDRP+GKy37L4MMPpm3btlxzw58B+MfjjzDjq6+49547ufeeOwEYdvf9dOjYMZsfKSvy8ws48tTzeODqi/HSUvrtfyiFPXrz7388QPcttqZvv70ZM/pZpkx4n/z8fFpt0ppjz0q6B6Z9OoHXn3uU/Px8LC+Pwaedz8Zt2mX3A9VRhD0EWMiJVWZ2PbCQZOkvB44nucvBjQDuvqCq9+53y9ua8bUBjPz1XjUXkhq99OmcbDchJxyzY7c6x8nP5iyrV0zYqutGDR6jQ2ewx6U/z6i0/wSSgFtlf6yISAURprChRxH0Dlm/iDQdMc7kCj2KYCMz+52ZDU+3+5jZESGPKSK5KcaLXKFHEfwVWAmUdQTOAv4U+JgikoMC3zImiNABdgt3vwFYBZDezya+PF9Esi/CCBs6wK40s1akt4lJV9OqcRVwEZFcEHoUwVXAaKCHmT1CcgPE9Zo+KyICcV7kCj2K4CUzex/oT5Kkn+fu34Q8pojkphgXAgt9R4NX3P0AkkW2K+8TEam1CONrmABrZi2BjYBOZtaeNeemDdXfa1xEZN0ijLChMtgzgPNJFnh5n+TUOMndDG4PdEwRyWEx9sEGGUXg7n9JZ3FdDeyUPv8ryS1jxoQ4pojkNk00WNtP3H2xmf0QGAjcB9wV+JgiIo1C6ABbkv48HLjX3UcCTfO+HyJSLxHOMwg+DnaWmd0DHARcb2YtCB/URSQHxThMK3SwOw54ETjE3RcBHYBGv+C2iDRG8eWwoScaLCPjzrHuPhuYXfU7RETWLcYMNs7794pIkxNhfFV/qIhIKMpgRSQK6iIQEQkkxplcCrAiEof44qsCrIjEIcL4qgArInFQH6yISCAx9sFqmJaISCDKYEUkDvElsAqwIhKHCOOrAqyIxEEXuUREAonxIpcCrIhEIcYMVqMIREQCUYAVEQlEXQQiEoUYuwgUYEUkCrrIJSISiDJYEZFAIoyvusglIhKKMlgRiUOEKawCrIhEQRe5REQC0UUuEZFAIoyvCrAiEokII6xGEYhIFKye/2qs32yQmU02sylmdsk6Xm9hZv9IX3/XzHrVVKcCrIg0eWaWDwwDDgX6AieaWd9KxU4DFrr7lsAtwPU11asAKyJRMKvfowa7A1Pcfaq7rwQeBwZXKjMYeCh9/hRwgFn1NTfaPtjXL9i70fe4mNkQdx+e7XbELobzeMyO3bLdhBrFcB7ro2VB/XphzWwIMCRj1/CM89UdmJHx2kxgj0pVlJdx99Vm9i3QEfimqmMqg62fITUXkVrQedwwdB6r4e7D3b1fxiP4LyMFWBERmAX0yNguSvets4yZFQBtgfnVVaoAKyICY4E+ZtbbzJoDJwAjKpUZAfw8ff4T4D/u7tVV2mj7YCORs/1dDUznccPQeayjtE/1bOBFIB94wN0nmdlQYJy7jwDuBx42synAApIgXC2rIQCLiEgdqYtARCQQBVgRkUAUYOvIzM43s40ytkeZWbssNqlJMLPLst2GhlD5+yVxUh9sKp2RYe5eWsvy04B+7l7lIGPZ8Mxsqbtvku12hJbN75eZFbj76oY+bi5q0hmsmfVKF3f4GzARuMLMxprZR2b2h7TMxmY20sw+NLOJZna8mZ0LbAq8amavpuWmmVmntM5PzOxeM5tkZi+ZWau0zG5p3R+Y2Y1mNjFbnz2UKs7XIDP71MzGm9ltZvZCWvYqM7so470TyxbQMLPnzOz99BwOSfddB7RKz98j2fh8IazjnP2etb9fB5vZmPQcPmlmm6T7p5nZDWY2wczeM7Mt0/0PmtndZjbOzD4zsyPS/fnpd6/se35Gun+Amb1pZiOAj7NyInKRuzfZB9ALKAX6AweTDHMxkl88LwD7Aj8G7s14T9v05zSgU8b+aUCntM7VwE7p/ieAn6XPJwJ7ps+vAyZm+xwEOKdrnS+S6YV90nP7BPBC+tpVwEUZZScCvdLnHdKfrdL9HdPtpdn+jA10zsq/X+n36g1g43T7/wFXZnzvLk+fn5xxbh8ERqff5T4kUz9bksz2+l1apgUwDugNDAC+A3pn+3zk0qNJZ7Cp6e7+DkmAPRj4HzAe2IbkizkBOMjMrjezfdz921rU+aW7f5A+fx/olfbPtnb3Men+RzfgZ2hMKpwvkv+8X7r75578r/57Les518w+BN4hmT3TJ0xzG4WavmP9SVZ4etvMPiAZ7L5ZxuuPZfzcM2P/E+5e6u6fA1NJvtMHAyen9bxLMpe+7Ny+5+5fbriPJZpokPzWhiS7utbd76lcwMx2AQ4D/mRmr7j70BrqXJHxvIQkC2sS3P2zzPMFvFJN8dVU7KZqCcmfq8CBJNn+MjN7rey1XFT5nJlZ5XNmwL/d/cSqqqjF87JtA85x9xcrHCA5598hG5Qy2DVeBE7N6NvqbmZdzGxTYJm7/x24EdglLb8EaF3byt19EbDEzMpW6KlxFkiM1nG+9iLJ4LdIi2QGiWmk5zMNML3T/W1J1t1cZmbbkGRwZVaZWbOAH6HBVfEdy/x+vQPsndG/urGZbZVRxfEZP8dk7D/WzPLSc785MJnke/6rsnNoZluZ2caBPlqTpww25e4vmdm2wJhkQAFLgZ8BWwI3mlkpsAr4VfqW4cBoM/va3fev5WFOA+5N63odqE13Q2x2YO3z1QkYaWbLgDdZEzieJvlzdRLJn6ufpftHA2ea2SckQeGdjPqHAx+Z2Xh3Pyn4p2kY6zpne5Lx/TKzU4DHzKxF+p7fseZ8tTezj0j+csr8BfYV8B7QBjjT3Zeb2X0k1wnGW/JFLwaOCvnhmjIN02pAZraJuy9Nn18CdHP387LcrAaV/il6kbsfkeWm5ASrYjiXmT1IcsHrqWy0SxLKYBvW4WZ2Kcl5nw6ckt3miEhIymBFRALRRS4RkUAUYEVEAlGAFREJRAG2CTKzknQ+/8R0XnudV21K57z/JH1+n619L/nMsgPMbK86HGOamXWq7f5KZZau57EqrI8gUh8KsE3T9+6+k7tvD6wEzsx80ZIbuq03dz/d3atbKGQAycQDkSZBAVbeBLasvJpSNasumZndYckqZC8DXcoqMrPXzKxf+nxQuvLTh2b2iiWrZJ0JXJBmz/uYWWczezo9xlgz2zt9b0dLViGblA6Mt5o+hK1j9a2M125J979iZp3TfVuY2ej0PW+mM8ZENiiNg23C0kz1UJKZU5BM0dze3b9Mg9S37r5bOnvobTN7CdgZ2Jpk8ZFCkqXtHqhUb2fgXmDftK4O7r7AzO4mWQ3rprTco8At7v6WmfUkmca5LfB74C13H2pmh5PMgKvJqekxWgFjzexpd58PbExy07oLzOzKtO6zSWaEnenun6fTl+8EBtbhNIpUSQG2aWplyWpKkGSw95P86Z65mtLBwA/K+ldJ1gfoQ7KE42PuXgJ8bWb/WUf9/YE3yupy9wVVtONAoG86NRmgjSVrQewLHJO+d6SZLazFZzrXzI5On5etvjWfZDnKf6T7/w48kx5jL+DJjGO3QGQDU4Btmr53950yd6SBJnM1papWXTpsA7YjD+jv7svX0ZZas/VbfcvT4y6qfA5ENjT1wUpVqlp16Q3g+LSPthuwroVu3gH2NbPe6Xs7pPsrr0D2EnBO2YaZ7ZQ+fQP4abrvUKB9DW2tbvWtPKAsC/8pSdfDYuBLMzs2PYaZ2Y41HENkvSnASlXuI+lfHW/JrW3uIfmL51ng8/S1v1FxeTwA3L2YZOX8ZyxZNLvsT/TngaPLLnIB5wL90otoH7NmNMMfSAL0JJKugq9qaOtooMCS1beuo+LqW98Bu6efYSBQtpbvScBpafsmAYNrcU5E1ovWIhARCUQZrIhIIAqwIiKBKMCKiASiACsiEogCrIhIIAqwIiKBKMCKiATy/wGiI89PFguT4gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adversarial part"
      ],
      "metadata": {
        "id": "9T8ADICaGSk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adversarial_pattern(image, label, model):\n",
        "    # gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "    # session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
        "    \n",
        "    image = tf.cast(image, tf.float32)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        prediction = model(image)\n",
        "        loss = tf.keras.losses.MSE(label, prediction)\n",
        "    \n",
        "    gradient = tape.gradient(loss, image)\n",
        "    \n",
        "    signed_grad = tf.sign(gradient)\n",
        "    \n",
        "    return signed_grad"
      ],
      "metadata": {
        "id": "zh6UKz-yGZli"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adversarial_attack(save_model = True, load_model = None, write_report = True, file_id = ''):\n",
        "\n",
        "  def write_values():\n",
        "    print(time.strftime('%H:%M:%S'), file = out_f)\n",
        "    print('window', window, 'overlap', overlap, 'decimation', decimation, file = out_f)\n",
        "    print('layers', dense1, lstm1, lstm2, lstm3, file = out_f)\n",
        "    print('oversample', oversample, file = out_f)\n",
        "    print('subj_train', permutation, file = out_f)\n",
        "    print('epochs', epochs, file = out_f)\n",
        "    if history is not None:\n",
        "      print('fit_accuracy', [round(x, 4) for x in history.history['accuracy']], file = out_f)\n",
        "    if history is not None and 'val_accuracy' in history.history:\n",
        "      print('fit_val_accuracy', [round(x, 4) for x in history.history['val_accuracy']], file = out_f)\n",
        "    print('subj_test', subjs_test, file = out_f)\n",
        "    print('test_accuracy', round(eval_metrics[1], 4), file = out_f)\n",
        "    print('permutation', perm_index + 1, file = out_f)\n",
        "    # print('iteration', iter + 1, file = out_f)\n",
        "    print('time_train', time_train, file = out_f)\n",
        "    print('time_test', time_test, file = out_f)\n",
        "    print(file = out_f)\n",
        "    out_f.flush()\n",
        "\n",
        "  if write_report:\n",
        "    output_file = time.strftime('%Y%m%d-%H%M%S') + '.txt'\n",
        "    out_f = open(working_dir + '/' + output_file, 'w')\n",
        "  # tensorboard stuff\n",
        "  #%rm -rf \"$log_dir_base\"\n",
        "  log_dir = log_dir_base + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "  for perm_index, permutation in enumerate(subjs_train_perm):\n",
        "    assert(type(permutation) == tuple)\n",
        "    assert(len(permutation) == 2)\n",
        "    assert(type(permutation[0]) == tuple)\n",
        "    assert(type(permutation[1]) == tuple)\n",
        "    assert(type(subjs_test) == tuple)\n",
        "    \n",
        "    if load_model is None:\n",
        "      model = create_model(window, dense1, lstm1, lstm2, lstm3)\n",
        "      # model training\n",
        "      x_data_train, y_data_train = partition_data(permutation[0])  # train subjects\n",
        "\n",
        "      perturbations = adversarial_pattern(x_data_train, y_data_train, model)\n",
        "      adversarial = x_data_train + perturbations * 0.5\n",
        "\n",
        "      print(\"Mean: \", np.mean(adversarial[:,:, 3]))\n",
        "      print(\"Deviation: \", np.std(adversarial[:,:, 3]))      \n",
        "\n",
        "      x_data_val, y_data_val = partition_data(permutation[1])  # validation subjects, can be None\n",
        "      print(f'### training with {len(adversarial)} inputs, {len(x_data_val) if x_data_val is not None else 0} validation')\n",
        "      tensorboard_callback = keras.callbacks.TensorBoard(log_dir + f'_{perm_index + 1}_{1}', histogram_freq = 1)\n",
        "      # train\n",
        "      start_time = time.monotonic()\n",
        "      history = model.fit(adversarial, y_data_train, epochs = epochs,\n",
        "        validation_data = (x_data_val, y_data_val) if x_data_val is not None else None,\n",
        "        callbacks = [tensorboard_callback])\n",
        "      time_train = time.monotonic() - start_time\n",
        "    else:\n",
        "      # model must match with dataset parameters\n",
        "      model = keras.models.load_model(load_model)\n",
        "      history = None\n",
        "      time_train = 0\n",
        "\n",
        "    # model test\n",
        "    x_data_test, y_data_test = partition_data(subjs_test)\n",
        "    print(f'### testing with {len(x_data_test)} inputs')\n",
        "    start_time = time.monotonic()\n",
        "    eval_metrics = model.evaluate(x_data_test, y_data_test)\n",
        "    time_test = time.monotonic() - start_time\n",
        "    if write_report:\n",
        "      write_values()\n",
        "    if save_model:\n",
        "      # save in both directory and h5 formats (we had problems with both of them sometimes)\n",
        "      model_file_name = f'{working_dir}/{file_id}model_w{window:04d}_o{overlap:03d}_d{decimation:03d}_e{epochs}_t{round(eval_metrics[1] * 10000)}'\n",
        "      #model.save(model_file_name)\n",
        "      model.save(model_file_name + '.h5')\n",
        "\n",
        "  if write_report:\n",
        "    out_f.close()"
      ],
      "metadata": {
        "id": "TLssv707DcmL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create and train model\n",
        "if __name__ == '__main__':\n",
        "  dense1 = 8\n",
        "  lstm1 = 8\n",
        "  lstm2 = 8\n",
        "  lstm3 = 8\n",
        "  # subjs_train_perm = ( ((0, 1, 2, 3, 4), ()), )  # final model\n",
        "  subjs_train_perm = ( ((0, 1, 2), ()), )  # final model\n",
        "  #subjs_train_perm = ( ((0, 1, 2, 3), (4,)), ((0, 1, 2, 4), (3,)), ((0, 1, 3, 4), (2,)), ((0, 2, 3, 4), (1,)), ((1, 2, 3, 4), (0,)) )  # cross-validation\n",
        "  subjs_test = (5, 6)\n",
        "  epochs = 5\n",
        "  iterations = 1  # repeat every test to cope with randomness\n",
        "  adversarial_attack()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H36V3fmGkk2",
        "outputId": "ccda3734-533a-424f-85cd-78738f8e412d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### creating model\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense1 (Dense)              (None, 1200, 8)           40        \n",
            "                                                                 \n",
            " norm (BatchNormalization)   (None, 1200, 8)           32        \n",
            "                                                                 \n",
            " lstm1 (LSTM)                (None, 1200, 8)           544       \n",
            "                                                                 \n",
            " drop2 (Dropout)             (None, 1200, 8)           0         \n",
            "                                                                 \n",
            " lstm2 (LSTM)                (None, 1200, 8)           544       \n",
            "                                                                 \n",
            " drop3 (Dropout)             (None, 1200, 8)           0         \n",
            "                                                                 \n",
            " lstm3 (LSTM)                (None, 8)                 544       \n",
            "                                                                 \n",
            " drop4 (Dropout)             (None, 8)                 0         \n",
            "                                                                 \n",
            " dense2 (Dense)              (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,731\n",
            "Trainable params: 1,715\n",
            "Non-trainable params: 16\n",
            "_________________________________________________________________\n",
            "Mean:  -0.0013451899\n",
            "Deviation:  1.0363206\n",
            "### training with 6220 inputs, 0 validation\n",
            "Epoch 1/5\n",
            "195/195 [==============================] - 33s 137ms/step - loss: 0.5691 - accuracy: 0.8182\n",
            "Epoch 2/5\n",
            "195/195 [==============================] - 15s 78ms/step - loss: 0.4188 - accuracy: 0.8465\n",
            "Epoch 3/5\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3906 - accuracy: 0.8579\n",
            "Epoch 4/5\n",
            "195/195 [==============================] - 16s 84ms/step - loss: 0.3779 - accuracy: 0.8596\n",
            "Epoch 5/5\n",
            "195/195 [==============================] - 15s 77ms/step - loss: 0.3997 - accuracy: 0.8466\n",
            "### testing with 2621 inputs\n",
            "82/82 [==============================] - 5s 41ms/step - loss: 0.4473 - accuracy: 0.8153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model test only\n",
        "attacked_model = tf.keras.models.load_model('/content/drive/MyDrive/dataset/epo5_eps0.5.h5')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  x_test, y_test = partition_data(subjs_test)\n",
        "  print(f'### testing with {len(x_test)} inputs')\n",
        "  eval_metrics = attacked_model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKKHA8t5zEIG",
        "outputId": "639235c7-cc3e-4868-8bce-d63a49eb4497"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### testing with 2621 inputs\n",
            "82/82 [==============================] - 5s 47ms/step - loss: 0.4578 - accuracy: 0.7959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create confusion matrix\n",
        "if __name__ == '__main__':\n",
        "  import pandas as pd\n",
        "  import seaborn\n",
        "  # y_pred = pretrained_model.predict_classes(x_data_test)\n",
        "  y_prob = attacked_model.predict(x_test) \n",
        "  y_pred = y_prob.argmax(axis=-1)\n",
        "  con_mat = tf.math.confusion_matrix(labels = y_test, predictions = y_pred).numpy()\n",
        "  con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis = 1)[:, np.newaxis], decimals = 2)\n",
        "  classes = ['resting', 'squat', 'stepper']\n",
        "  con_mat_df = pd.DataFrame(con_mat_norm, index = classes, columns = classes)\n",
        "  #con_mat_df['squat']['stepper']=0.11  # fix sum not 1 due to rounding\n",
        "  #print(con_mat_df)\n",
        "  figure = plt.figure(figsize = (5, 5))\n",
        "  seaborn.heatmap(con_mat_df, annot = True, cmap = plt.cm.Blues)\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(working_dir + '/confusion_matrix.eps', format='eps')\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "EytE_FER76cI",
        "outputId": "ba58e9c7-ebb4-4093-f37f-125531729aa0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82/82 [==============================] - 4s 33ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}